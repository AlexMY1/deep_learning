{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html(url:str, static=True) :\n",
    "    \"\"\"\n",
    "    Load a BeautifulSoup instance over an HTML page.\n",
    "    If static=False, we use Selenium to load the page.\n",
    "    If static=True, we use requests to load the page.\n",
    "    Input :\n",
    "    url: str, url of the page\n",
    "    static: bool\n",
    "    \"\"\"\n",
    "\n",
    "    if static :\n",
    "        response = requests.get(url)\n",
    "        return BeautifulSoup(response.content, \"html.parser\")\n",
    "    else :\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(url)\n",
    "        sleep(5)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        driver.quit()\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_search(search_url) :\n",
    "    urls=[]\n",
    "    soup = load_html(search_url, False)\n",
    "    for tag in soup.find_all(\"a\", href=True) :\n",
    "        if \"from_search\" in tag[\"href\"] : urls.append(tag[\"href\"])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_most_read(slideshow: dict) -> list :\n",
    "    \"\"\"\n",
    "    Construct a one-hot vector of length nb_slides. Put a 1 at the index of slides most read.\n",
    "    Inputs :\n",
    "    slideshow: dict, metadata of the slideshow (extracted with BeautifulSoup)\n",
    "    Ouputs :\n",
    "    isMostRead: list, one-hot vector of nb_slides \n",
    "    \"\"\"\n",
    "    nb_slides = slideshow[\"totalSlides\"]\n",
    "    indexMostRead = [list(slide.items())[0][1] for slide in slideshow[\"topReadSlides\"]]\n",
    "    isMostRead = [1 if k in indexMostRead else 0 for k in range(nb_slides)]\n",
    "    return isMostRead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(url:str) :\n",
    "    \"\"\"\n",
    "    The scrap function slides from SlideShare.net and then creates a folder which contains all slides in jpg format.\n",
    "    Input :\n",
    "    url : a str-like, contains the url of the download page of the slides\n",
    "    \"\"\"\n",
    "    #html request\n",
    "    soup = load_html(url)\n",
    "\n",
    "    #The script tag with id \"__NETX_DATA__\" contains with a json format some meta informations.\n",
    "    data = json.loads(soup.select_one(\"#__NEXT_DATA__\").text)\n",
    "    slideshow =  data[\"props\"][\"pageProps\"][\"slideshow\"]\n",
    "\n",
    "    if type(slideshow[\"transcript\"]) == str : transcript = [slideshow[\"transcript\"]]\n",
    "    else : transcript = slideshow[\"transcript\"]\n",
    "\n",
    "    meta_data = { \"id\": slideshow[\"id\"],\n",
    "                  \"presentation_url\": slideshow[\"canonicalUrl\"],\n",
    "                  \"title\": slideshow[\"title\"],\n",
    "                  \"author\": slideshow[\"username\"],\n",
    "                  \"date\": slideshow[\"createdAt\"][:10], #AAAA-MM-JJ\n",
    "                  \"len\": slideshow[\"totalSlides\"],\n",
    "                  \"description\": slideshow[\"description\"],\n",
    "                  \"lang\": slideshow[\"language\"],\n",
    "                  \"dim\": slideshow[\"slideDimensions\"],\n",
    "                  \"like\": slideshow[\"likes\"],\n",
    "                  \"view\": slideshow[\"views\"],\n",
    "                  \"transcript\": transcript,\n",
    "                  \"mostRead\": extract_most_read(slideshow),\n",
    "                  }\n",
    "    #img_urls\n",
    "    slides = slideshow[\"slides\"]\n",
    "    sizes = slides[\"imageSizes\"][1]\n",
    "    img_url = f\"{slides['host']}/{slides['imageLocation']}/{sizes['quality']}/{slideshow['title']}-\" + \"{}\" + f\"-{sizes['width']}.{sizes['format']}\"\n",
    "\n",
    "    #folder creation\n",
    "    folder_name = str(slideshow[\"id\"])\n",
    "    if not os.path.exists(DATA_PATH) : os.mkdir(DATA_PATH)\n",
    "    if os.path.exists(DATA_PATH+\"/\"+folder_name) : shutil.rmtree(DATA_PATH+\"/\"+folder_name)\n",
    "    os.mkdir(DATA_PATH+\"/\"+folder_name)\n",
    "    \n",
    "    #For each image\n",
    "    for i in range(slideshow[\"totalSlides\"]):\n",
    "        response = requests.get(img_url.format(i+1))\n",
    "        sleep(5)\n",
    "        with open(f\"{DATA_PATH}/{folder_name}/slide_{i}.jpg\", \"wb\") as f :\n",
    "            f.write(response.content)\n",
    "            \n",
    "    with open(f\"{DATA_PATH}/{folder_name}/{folder_name.lower()}.json\", \"w\") as f :\n",
    "        json.dump(meta_data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get urls of slideshows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of pages (1 page = 18 slideshows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_i is the url format of the content search page in French on slideshare\n",
    "#the first page is page_i.format(1), the second page is page_2.format(2) etc...\n",
    "#nb pages is the number of the last content search page that we scrap.\n",
    "page_i = \"https://fr.slideshare.net/search?searchfrom=header&q=espa%C3%B1ol&language=es&page={}\"\n",
    "nb_pages = 30\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this to scrap urls of slideshows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execution of the scraper\n",
    "for i in range(1,nb_pages+1) :\n",
    "    urls.extend(extract_urls_from_search(page_i.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"urls.csv\", \"w\", newline=\"\") as f:\n",
    "    for url in urls : f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already scraped urls you can get them with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "with open(\"urls.csv\", \"r\") as f :\n",
    "    for url in f.readlines() : urls.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you begin the scraping execute this cell and execute the scraping cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_slideshow = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to resume after a break of the loop, execute this cell and relunch the scraping cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"last_slideshow.txt\") as f : last_slideshow = int(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping cell :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 90/540 [2:32:26<12:42:13, 101.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(last_slideshow\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(urls))) :\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m :\n\u001b[1;32m----> 4\u001b[0m         \u001b[43mscrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m         sleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e : \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyError catched : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m, in \u001b[0;36mscrap\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(slideshow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalSlides\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     44\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(img_url\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 45\u001b[0m     sleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/slide_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f :\n\u001b[0;32m     47\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Scraping cell\n",
    "for j in tqdm(range(last_slideshow+1, len(urls))) :\n",
    "    try :\n",
    "        scrap(urls[j])\n",
    "        sleep(3)\n",
    "    except KeyError as e : print(f\"KeyError catched : {e}\")\n",
    "    except requests.exceptions.ConnectionError as e : print(f\"ConnectionError catched : {e}\")\n",
    "    except requests.exceptions.HTTPError as e : print(f\"HTTPError catched : {e}\")\n",
    "    last_slideshow+=1\n",
    "    with open(\"last_slideshow.txt\", \"w\") as f : f.write(str(last_slideshow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the Q/A generation, please run this cell :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verif integrity\n",
    "listdir = os.listdir(DATA_PATH)\n",
    "for dir in listdir : \n",
    "    meta_data_path = f\"{DATA_PATH}/{dir}/{dir}.json\"\n",
    "    if not os.path.exists(meta_data_path) :\n",
    "        print(f\"Can't find meta-data for {dir}\")\n",
    "        shutil.rmtree(f\"{DATA_PATH}/{dir}\")\n",
    "    else :\n",
    "        with open(meta_data_path, \"r\") as f:\n",
    "            meta_data = json.load(f)\n",
    "        if type(meta_data[\"id\"]) != str : print(f\"{dir} : id type error\")\n",
    "        if type(meta_data[\"presentation_url\"]) != str : print(f\"{dir}: presentation_url type error\")\n",
    "        if type(meta_data[\"title\"]) != str : print(f\"{dir} : title type error\")\n",
    "        if type(meta_data[\"author\"]) != str: print(f\"{dir} : author type error\")\n",
    "        if type(meta_data[\"date\"]) != str : print(f\"{dir} : date type error\")\n",
    "        if type(meta_data[\"len\"]) != int : print(f\"{dir} : len type error\")\n",
    "        if type(meta_data[\"description\"]) != str : print(f\"{dir} : description type error\")\n",
    "        if type(meta_data[\"lang\"]) != str : print(f\"{dir} :lang  type error\")\n",
    "        if type(meta_data[\"dim\"]) != dict : print(f\"{dir} : dim type error\")\n",
    "        if type(meta_data[\"like\"]) != int : print(f\"{dir} : like type error\")\n",
    "        if type(meta_data[\"view\"]) != int : print(f\"{dir} : view type error\")\n",
    "        if type(meta_data[\"transcript\"]) != list : print(f\"{dir} : transcript type error\")\n",
    "        if type(meta_data[\"mostRead\"]) != list : print(f\"{dir} : mostRead type error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell returns nothing, you can go to generate_qa.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
